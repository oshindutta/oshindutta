# ğŸ‘‹ Hi, I'm Oshin Dutta !
Ph.D. Scholar in AI at IIT Delhi â€¢ Researcher in Efficient AI â€¢ Enthusiast in LLMs, NAS, and Model Compression

[ğŸŒ Website](https://oshindutta.github.io/) â€¢ [ğŸ“« Email](mailto:oshin.dutta@ee.iitd.ac.in)

---

## ğŸ”¬ Current Role

**Ph.D. Scholar, IIT Delhi**  
_Working on Efficient AI & Accelerating LLMs_  
Collaborated with [Samsung Research](#) and [Cadence India](#)

- ğŸ§  **VTrans** â€“ 10Ã— speed-up for LLM fine-tuning + 50% compression
- ğŸš€ **TVA-prune** â€“ 60% GPU inference speed-up for LLaMA/Mistral
- ğŸ¤– **DCA-NAS** â€“ 5Ã— faster hardware-aware NAS on distributed GPUs

---

## ğŸ§‘â€ğŸ”¬ Past Experience

**Student Researcher, IIT Dhanbad**  
- Worked on tempo and rhythm extraction in polyphonic music using ML  
- Published findings in IEEE conference  

**Intern, Aerospace Dept., IISc Bangalore**  
- Designed algorithms for fuel-efficient lunar landings  
- Benchmarked efficiency on TMS320C6748 DSP  

---

## ğŸ› ï¸ Skills

- **Languages**: Python, C, Java, MATLAB  
- **Frameworks**: PyTorch, TensorFlow, OpenCV  
- **AI**: CNNs, RNNs, GANs, LLMs, ViTs, Multimodal, NAS  
- **Research Interests**: Efficient AI, Model Compression, Pruning, Quantization, NAS  
- **Other**: LoRA, Few-shot Learning, Post-Training Quantization, Deployment

---

## ğŸ“« Connect with Me

[LinkedIn](https://linkedin.com/in/oshindutta) â€¢ [Twitter](https://x.com/dutta_oshin) â€¢ [GitHub](https://github.com/oshindutta)


<!--
**oshindutta/oshindutta** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ğŸ”­ Iâ€™m currently working on ...
- ğŸŒ± Iâ€™m currently learning ...
- ğŸ‘¯ Iâ€™m looking to collaborate on ...
- ğŸ¤” Iâ€™m looking for help with ...
- ğŸ’¬ Ask me about ...
- ğŸ“« How to reach me: ...
- ğŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->
